{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunariver/newproject/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_GPT_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9s_Popr6eWA",
        "outputId": "ea1f6836-381c-4779-aa0c-92bf6396e9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-18 02:35:21--  https://raw.githubusercontent.com/shop2world/data/master/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-10-18 02:35:21 (29.4 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "데이터셋의 글자 수:  1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 다운로드하기\n",
        "!wget https://raw.githubusercontent.com/shop2world/data/master/input.txt\n",
        "\n",
        "# 데이터셋을 읽어서 확인하기\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"데이터셋의 글자 수: \", len(text))\n",
        "\n",
        "# 처음 1000자를 확인해보기\n",
        "print(text[:1000])\n",
        "\n",
        "#설명:\n",
        "#위 코드는 작은 규모의 셰익스피어 텍스트 데이터셋을 다운로드하고, 해당 데이터셋을 읽어서 내용을 확인하는 코드입니다.\n",
        "#먼저 `wget` 명령어를 사용하여 데이터셋을 다운로드합니다. 그런 다음 `open` 함수를 사용하여 파일을 열고, `read` 메서드를 통해 파일 내용을 읽어옵니다.\n",
        "#읽어온 데이터의 길이를 확인하고, 처음 1000자를 출력하여 데이터의 내용을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 주어진 텍스트에서 발생하는 모든 고유한 문자 확인\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZLC48rI6kIT",
        "outputId": "9bef6839-bbc0-471d-fe45-954c726eb811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열을 정수 리스트로 변환하고(토큰), 그 반대로 정수 리스트를 문자열로 변환하는 과정\n",
        "import string\n",
        "\n",
        "# 확장된 문자 리스트 생성\n",
        "chars = sorted(list(set(text + string.ascii_letters + string.digits + string.punctuation)))\n",
        "print(len(chars))  # chars 리스트의 크기 출력\n",
        "\n",
        "# 인코딩 및 디코딩 함수 정의\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s if c in stoi]  # 인코딩 시 stoi에 없는 문자는 무시\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hello shop2world\"))\n",
        "print(decode(encode(\"hello shop2world\")))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "7d75c77b-4134-481f-f5aa-f0c3a0a97d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n",
            "[73, 70, 77, 77, 80, 1, 84, 73, 80, 81, 19, 88, 80, 83, 77, 69]\n",
            "hello shop2world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋을 인코딩하여 torch.Tensor로 저장하는 과정 (비정형데이터->정형데이터)\n",
        "# 데이터 전처리 과정은 텍스트 데이터를 딥러닝 모델에 입력으로 사용하기 위해 필요한 단계\n",
        "import torch #  PyTorch는 딥러닝 프레임워크: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)#인코딩된 정수 리스트를 torch.Tensor로 변환\n",
        "print(data.shape, data.dtype)#data.shape는 텐서의 형태(크기), data.dtype는 텐서의 데이터 타입\n",
        "print(data[:1000]) #  텍스트의 처음 1000개 문자가 GPT 모델에게 어떻게 보일지를 나타냅니다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "c491f09c-1b49-4467-c482-40bd4a46732d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([39, 74, 83, 84, 85,  1, 36, 74, 85, 74, 91, 70, 79, 27,  0, 35, 70, 71,\n",
            "        80, 83, 70,  1, 88, 70,  1, 81, 83, 80, 68, 70, 70, 69,  1, 66, 79, 90,\n",
            "         1, 71, 86, 83, 85, 73, 70, 83, 13,  1, 73, 70, 66, 83,  1, 78, 70,  1,\n",
            "        84, 81, 70, 66, 76, 15,  0,  0, 34, 77, 77, 27,  0, 52, 81, 70, 66, 76,\n",
            "        13,  1, 84, 81, 70, 66, 76, 15,  0,  0, 39, 74, 83, 84, 85,  1, 36, 74,\n",
            "        85, 74, 91, 70, 79, 27,  0, 58, 80, 86,  1, 66, 83, 70,  1, 66, 77, 77,\n",
            "         1, 83, 70, 84, 80, 77, 87, 70, 69,  1, 83, 66, 85, 73, 70, 83,  1, 85,\n",
            "        80,  1, 69, 74, 70,  1, 85, 73, 66, 79,  1, 85, 80,  1, 71, 66, 78, 74,\n",
            "        84, 73, 32,  0,  0, 34, 77, 77, 27,  0, 51, 70, 84, 80, 77, 87, 70, 69,\n",
            "        15,  1, 83, 70, 84, 80, 77, 87, 70, 69, 15,  0,  0, 39, 74, 83, 84, 85,\n",
            "         1, 36, 74, 85, 74, 91, 70, 79, 27,  0, 39, 74, 83, 84, 85, 13,  1, 90,\n",
            "        80, 86,  1, 76, 79, 80, 88,  1, 36, 66, 74, 86, 84,  1, 46, 66, 83, 68,\n",
            "        74, 86, 84,  1, 74, 84,  1, 68, 73, 74, 70, 71,  1, 70, 79, 70, 78, 90,\n",
            "         1, 85, 80,  1, 85, 73, 70,  1, 81, 70, 80, 81, 77, 70, 15,  0,  0, 34,\n",
            "        77, 77, 27,  0, 56, 70,  1, 76, 79, 80, 88,  8, 85, 13,  1, 88, 70,  1,\n",
            "        76, 79, 80, 88,  8, 85, 15,  0,  0, 39, 74, 83, 84, 85,  1, 36, 74, 85,\n",
            "        74, 91, 70, 79, 27,  0, 45, 70, 85,  1, 86, 84,  1, 76, 74, 77, 77,  1,\n",
            "        73, 74, 78, 13,  1, 66, 79, 69,  1, 88, 70,  8, 77, 77,  1, 73, 66, 87,\n",
            "        70,  1, 68, 80, 83, 79,  1, 66, 85,  1, 80, 86, 83,  1, 80, 88, 79,  1,\n",
            "        81, 83, 74, 68, 70, 15,  0, 42, 84,  8, 85,  1, 66,  1, 87, 70, 83, 69,\n",
            "        74, 68, 85, 32,  0,  0, 34, 77, 77, 27,  0, 47, 80,  1, 78, 80, 83, 70,\n",
            "         1, 85, 66, 77, 76, 74, 79, 72,  1, 80, 79,  8, 85, 28,  1, 77, 70, 85,\n",
            "         1, 74, 85,  1, 67, 70,  1, 69, 80, 79, 70, 27,  1, 66, 88, 66, 90, 13,\n",
            "         1, 66, 88, 66, 90,  2,  0,  0, 52, 70, 68, 80, 79, 69,  1, 36, 74, 85,\n",
            "        74, 91, 70, 79, 27,  0, 48, 79, 70,  1, 88, 80, 83, 69, 13,  1, 72, 80,\n",
            "        80, 69,  1, 68, 74, 85, 74, 91, 70, 79, 84, 15,  0,  0, 39, 74, 83, 84,\n",
            "        85,  1, 36, 74, 85, 74, 91, 70, 79, 27,  0, 56, 70,  1, 66, 83, 70,  1,\n",
            "        66, 68, 68, 80, 86, 79, 85, 70, 69,  1, 81, 80, 80, 83,  1, 68, 74, 85,\n",
            "        74, 91, 70, 79, 84, 13,  1, 85, 73, 70,  1, 81, 66, 85, 83, 74, 68, 74,\n",
            "        66, 79, 84,  1, 72, 80, 80, 69, 15,  0, 56, 73, 66, 85,  1, 66, 86, 85,\n",
            "        73, 80, 83, 74, 85, 90,  1, 84, 86, 83, 71, 70, 74, 85, 84,  1, 80, 79,\n",
            "         1, 88, 80, 86, 77, 69,  1, 83, 70, 77, 74, 70, 87, 70,  1, 86, 84, 27,\n",
            "         1, 74, 71,  1, 85, 73, 70, 90,  0, 88, 80, 86, 77, 69,  1, 90, 74, 70,\n",
            "        77, 69,  1, 86, 84,  1, 67, 86, 85,  1, 85, 73, 70,  1, 84, 86, 81, 70,\n",
            "        83, 71, 77, 86, 74, 85, 90, 13,  1, 88, 73, 74, 77, 70,  1, 74, 85,  1,\n",
            "        88, 70, 83, 70,  0, 88, 73, 80, 77, 70, 84, 80, 78, 70, 13,  1, 88, 70,\n",
            "         1, 78, 74, 72, 73, 85,  1, 72, 86, 70, 84, 84,  1, 85, 73, 70, 90,  1,\n",
            "        83, 70, 77, 74, 70, 87, 70, 69,  1, 86, 84,  1, 73, 86, 78, 66, 79, 70,\n",
            "        77, 90, 28,  0, 67, 86, 85,  1, 85, 73, 70, 90,  1, 85, 73, 74, 79, 76,\n",
            "         1, 88, 70,  1, 66, 83, 70,  1, 85, 80, 80,  1, 69, 70, 66, 83, 27,  1,\n",
            "        85, 73, 70,  1, 77, 70, 66, 79, 79, 70, 84, 84,  1, 85, 73, 66, 85,  0,\n",
            "        66, 71, 71, 77, 74, 68, 85, 84,  1, 86, 84, 13,  1, 85, 73, 70,  1, 80,\n",
            "        67, 75, 70, 68, 85,  1, 80, 71,  1, 80, 86, 83,  1, 78, 74, 84, 70, 83,\n",
            "        90, 13,  1, 74, 84,  1, 66, 84,  1, 66, 79,  0, 74, 79, 87, 70, 79, 85,\n",
            "        80, 83, 90,  1, 85, 80,  1, 81, 66, 83, 85, 74, 68, 86, 77, 66, 83, 74,\n",
            "        84, 70,  1, 85, 73, 70, 74, 83,  1, 66, 67, 86, 79, 69, 66, 79, 68, 70,\n",
            "        28,  1, 80, 86, 83,  0, 84, 86, 71, 71, 70, 83, 66, 79, 68, 70,  1, 74,\n",
            "        84,  1, 66,  1, 72, 66, 74, 79,  1, 85, 80,  1, 85, 73, 70, 78,  1, 45,\n",
            "        70, 85,  1, 86, 84,  1, 83, 70, 87, 70, 79, 72, 70,  1, 85, 73, 74, 84,\n",
            "         1, 88, 74, 85, 73,  0, 80, 86, 83,  1, 81, 74, 76, 70, 84, 13,  1, 70,\n",
            "        83, 70,  1, 88, 70,  1, 67, 70, 68, 80, 78, 70,  1, 83, 66, 76, 70, 84,\n",
            "        27,  1, 71, 80, 83,  1, 85, 73, 70,  1, 72, 80, 69, 84,  1, 76, 79, 80,\n",
            "        88,  1, 42,  0, 84, 81, 70, 66, 76,  1, 85, 73, 74, 84,  1, 74, 79,  1,\n",
            "        73, 86, 79, 72, 70, 83,  1, 71, 80, 83,  1, 67, 83, 70, 66, 69, 13,  1,\n",
            "        79, 80, 85,  1, 74, 79,  1, 85, 73, 74, 83, 84, 85,  1, 71, 80, 83,  1,\n",
            "        83, 70, 87, 70, 79, 72, 70, 15,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 훈련 세트(train set)와 검증 세트(validation set)로 분할\n",
        "n = int(0.9*len(data)) # 데이터의 길이의 90%에 해당하는 값을 n 변수에 저장\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 5\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pJt4bQfiBYc",
        "outputId": "96738a7d-9d68-4a4c-9e21-fa24a5b8e5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([39, 74, 83, 84, 85,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#트레인 , test 분리\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"입력값이 {context} 일경우 대상은: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "c5c0c91b-e803-4a7c-be2c-062f54f1a47f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력값이 tensor([39]) 일경우 대상은: 74\n",
            "입력값이 tensor([39, 74]) 일경우 대상은: 83\n",
            "입력값이 tensor([39, 74, 83]) 일경우 대상은: 84\n",
            "입력값이 tensor([39, 74, 83, 84]) 일경우 대상은: 85\n",
            "입력값이 tensor([39, 74, 83, 84, 85]) 일경우 대상은: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(777)\n",
        "batch_size = 4 # 병렬로 처리할 독립적인 시퀀스의 개수\n",
        "block_size = 8 # 예측에 사용할 수 있는 최대 문맥 길이\n",
        "\n",
        "def get_batch(split):\n",
        "    # 입력(x)과 대상(y)의 작은 배치 데이터를 생성합니다.\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))#예)[1,4,3,0]\n",
        "    #선택 가능한 범위에서 batch_size 개수만큼의 임의의 정수를 선택하여 반환\n",
        "    #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "# 'train' 데이터로 get_batch 함수를 호출하여 입력(xb)과 대상(yb)의 작은 배치 데이터를 생성합니다.\n",
        "xb, yb = get_batch('train')\n",
        "# 생성된 입력(xb)의 shape와 내용을 출력합니다.\n",
        "print('입력값:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "# 생성된 대상(yb)의 shape와 내용을 출력합니다.\n",
        "print('대상(targets):')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # 배치 차원\n",
        "    for t in range(block_size): # 시간 차원\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"입력값이 {context.tolist()} 일경우 대상값: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "83d65b33-613a-47fe-863c-a081c92a1d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력값:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 73, 66, 87, 70,  1, 67, 77],\n",
            "        [70, 70, 69, 84, 13,  1, 74, 79],\n",
            "        [52, 70, 68, 80, 79, 69,  1, 44],\n",
            "        [79, 27,  0, 70, 87, 70, 83, 90]])\n",
            "대상(targets):\n",
            "torch.Size([4, 8])\n",
            "tensor([[73, 66, 87, 70,  1, 67, 77, 70],\n",
            "        [70, 69, 84, 13,  1, 74, 79,  1],\n",
            "        [70, 68, 80, 79, 69,  1, 44, 70],\n",
            "        [27,  0, 70, 87, 70, 83, 90,  1]])\n",
            "----\n",
            "입력값이 [1] 일경우 대상값: 73\n",
            "입력값이 [1, 73] 일경우 대상값: 66\n",
            "입력값이 [1, 73, 66] 일경우 대상값: 87\n",
            "입력값이 [1, 73, 66, 87] 일경우 대상값: 70\n",
            "입력값이 [1, 73, 66, 87, 70] 일경우 대상값: 1\n",
            "입력값이 [1, 73, 66, 87, 70, 1] 일경우 대상값: 67\n",
            "입력값이 [1, 73, 66, 87, 70, 1, 67] 일경우 대상값: 77\n",
            "입력값이 [1, 73, 66, 87, 70, 1, 67, 77] 일경우 대상값: 70\n",
            "입력값이 [70] 일경우 대상값: 70\n",
            "입력값이 [70, 70] 일경우 대상값: 69\n",
            "입력값이 [70, 70, 69] 일경우 대상값: 84\n",
            "입력값이 [70, 70, 69, 84] 일경우 대상값: 13\n",
            "입력값이 [70, 70, 69, 84, 13] 일경우 대상값: 1\n",
            "입력값이 [70, 70, 69, 84, 13, 1] 일경우 대상값: 74\n",
            "입력값이 [70, 70, 69, 84, 13, 1, 74] 일경우 대상값: 79\n",
            "입력값이 [70, 70, 69, 84, 13, 1, 74, 79] 일경우 대상값: 1\n",
            "입력값이 [52] 일경우 대상값: 70\n",
            "입력값이 [52, 70] 일경우 대상값: 68\n",
            "입력값이 [52, 70, 68] 일경우 대상값: 80\n",
            "입력값이 [52, 70, 68, 80] 일경우 대상값: 79\n",
            "입력값이 [52, 70, 68, 80, 79] 일경우 대상값: 69\n",
            "입력값이 [52, 70, 68, 80, 79, 69] 일경우 대상값: 1\n",
            "입력값이 [52, 70, 68, 80, 79, 69, 1] 일경우 대상값: 44\n",
            "입력값이 [52, 70, 68, 80, 79, 69, 1, 44] 일경우 대상값: 70\n",
            "입력값이 [79] 일경우 대상값: 27\n",
            "입력값이 [79, 27] 일경우 대상값: 0\n",
            "입력값이 [79, 27, 0] 일경우 대상값: 70\n",
            "입력값이 [79, 27, 0, 70] 일경우 대상값: 87\n",
            "입력값이 [79, 27, 0, 70, 87] 일경우 대상값: 70\n",
            "입력값이 [79, 27, 0, 70, 87, 70] 일경우 대상값: 83\n",
            "입력값이 [79, 27, 0, 70, 87, 70, 83] 일경우 대상값: 90\n",
            "입력값이 [79, 27, 0, 70, 87, 70, 83, 90] 일경우 대상값: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # xb는 Transformer 모델에 입력으로 사용되는 데이터"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhSCUgOtZ-O3",
        "outputId": "dd9490fe-3321-4f96-a71a-4e12bca8ff83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 73, 66, 87, 70,  1, 67, 77],\n",
            "        [70, 70, 69, 84, 13,  1, 74, 79],\n",
            "        [52, 70, 68, 80, 79, 69,  1, 44],\n",
            "        [79, 27,  0, 70, 87, 70, 83, 90]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn #파이토치 Neural Networks\n",
        "from torch.nn import functional as F # 신경망 모델 함수들을 제공\n",
        "torch.manual_seed(777)\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # 각 토큰은 다음 토큰에 대한 로짓을 직접 읽어옵니다 (룩업 테이블에서)\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      # idx와 targets는 모두 (B, T) 형태의 정수 텐서입니다\n",
        "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx는 현재 문맥의 인덱스로 이루어진 (B, T) 배열입니다\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            # 예측 결과를 얻습니다\n",
        "            logits = logits[:, -1, :]# (B, C) 형태로 변환됨\n",
        "            # 확률을 얻기 위해 softmax를 적용합니다\n",
        "            probs = F.softmax(logits, dim=-1)# (B, C)\n",
        "            # 확률을 얻기 위해 softmax를 적용합니다\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)# (B, C)\n",
        "            # 샘플링된 인덱스를 현재 시퀀스에 추가합니다\n",
        "            idx = torch.cat((idx, idx_next), dim=1)# (B, T+1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "vocab_size = len(chars) #chars 리스트의 길이를 vocab_size로 사용\n",
        "m = LanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkWCH5qYgpBo",
        "outputId": "85b70a8c-bd8f-4e3c-b4c7-315c9c8e732f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 96])\n",
            "tensor(5.1830, grad_fn=<NllLossBackward0>)\n",
            "\n",
            ";L3?qhavOPYQl<ZI8@\\M1??^gkte{S(&Q>?cF#GI_o67mEzGKNL9\\yj<L XF kp~V2@NG-goSUevy/K}q>&!vL8jEf!xWv`=k\\o;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch 옵티마이저\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # 많이 돌리면 ...\n",
        "\n",
        "    # 데이터의 미니배치를 추출합니다.\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # 손실을 평가합니다.\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "cfe8f8ad-a6dd-4bef-9d25-7d5f7c6b4656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4121220111846924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "4fa557f1-acf3-4420-a4b0-19eeec12e8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aur;\n",
            "SINond he ir sishu, witeife; Whes! a, natherbenthance r hangh\n",
            "Pre f cknthecher f in-esareetocag GAUThanspurthathee ter meafls. beer o:\n",
            "KE:\n",
            "CHa Justh;\n",
            "Thooswillacot(&GIOLUES:\n",
            "\n",
            "\n",
            "Asields ffouler ie thennd su,\n",
            "Assth ld taim t d pe w\n",
            "\n",
            "LENoieay Hashasoney?\"hiz, a t t t h ds s b n if tinghey arg po ad f breeey RARauns\\IUMo s he hifeanouly plld,\n",
            "Sousen hre\n",
            "Th,\n",
            "Mithir s omouswinc! lifomelito, heeds ke r,\n",
            "Thele\n",
            "LENGea r he M`4ja an fet I lthe, pill be m; iny gomoo ves,\n",
            "\n",
            "PRIOWhar; ba o st thioura Coa \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn #파이토치 Neural Networks\n",
        "from torch.nn import functional as F # 신경망 모델 함수들을 제공\n",
        "torch.manual_seed(777)\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# 데이터셋 다운로드하기\n",
        "!wget https://raw.githubusercontent.com/shop2world/data/master/input.txt\n",
        "\n",
        "# 데이터셋을 읽어서 확인하기\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 문자열을 정수 리스트로 변환하고, 그 반대로 정수 리스트를 문자열로 변환하는 과정\n",
        "import string\n",
        "\n",
        "# 확장된 문자 리스트 생성\n",
        "chars = sorted(list(set(text + string.ascii_letters + string.digits + string.punctuation)))\n",
        "print(len(chars))  # chars 리스트의 크기 출력\n",
        "\n",
        "# 인코딩 및 디코딩 함수 정의\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s if c in stoi]  # 인코딩 시 stoi에 없는 문자는 무시\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "#트레인 , test 분리\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    # 입력(x)과 대상(y)의 작은 배치 데이터를 생성합니다.\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))#예)[1,4,3,0]\n",
        "    #선택 가능한 범위에서 batch_size 개수만큼의 임의의 정수를 선택하여 반환\n",
        "    #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "#새로 추가한 부분\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # 각 토큰은 다음 토큰에 대한 로짓을 직접 읽어옵니다 (룩업 테이블에서)\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      # idx와 targets는 모두 (B, T) 형태의 정수 텐서입니다\n",
        "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx는 현재 문맥의 인덱스로 이루어진 (B, T) 배열입니다\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            # 예측 결과를 얻습니다\n",
        "            logits = logits[:, -1, :]# (B, C) 형태로 변환됨\n",
        "            # 확률을 얻기 위해 softmax를 적용합니다\n",
        "            probs = F.softmax(logits, dim=-1)# (B, C)\n",
        "            # 확률을 얻기 위해 softmax를 적용합니다\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)# (B, C)\n",
        "            # 샘플링된 인덱스를 현재 시퀀스에 추가합니다\n",
        "            idx = torch.cat((idx, idx_next), dim=1)# (B, T+1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#추가한 부분\n",
        "model = LanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "# PyTorch 옵티마이저\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "vocab_size = len(chars) #chars 리스트의 길이를 vocab_size로 사용\n",
        "m = LanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f460270c-f08b-441e-f411-60a807e295db",
        "id": "yU1W4qWpXzWU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-18 02:35:40--  https://raw.githubusercontent.com/shop2world/data/master/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-10-18 02:35:40 (24.1 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "96\n",
            "\n",
            "gN4xS`:R2JWUr_9GFsm,,[j*pJ0NV}lhv\n",
            "ou)_B>lb6<8ThB[] \"5!*=%Z:SInY6'.'<9r7l|h_K&`aCCL>V1b56~_eZz/S0:GcF\n",
            "torch.Size([256, 96])\n",
            "tensor(5.2187, grad_fn=<NllLossBackward0>)\n",
            "step 0: train loss 5.0909, val loss 5.0970\n",
            "step 300: train loss 4.7353, val loss 4.7371\n",
            "step 600: train loss 4.3972, val loss 4.4062\n",
            "step 900: train loss 4.1033, val loss 4.1241\n",
            "step 1200: train loss 3.8532, val loss 3.8667\n",
            "step 1500: train loss 3.6246, val loss 3.6392\n",
            "step 1800: train loss 3.4336, val loss 3.4473\n",
            "step 2100: train loss 3.2700, val loss 3.2800\n",
            "step 2400: train loss 3.1233, val loss 3.1441\n",
            "step 2700: train loss 3.0152, val loss 3.0398\n",
            "\n",
            "por&Q=xuP>bV4n% 28=C].rNJhChn)f\"m.O7\"nRd`3+Vf$:~:cs'Chqk:OqsKk6jFFoHhkUr[$^3:WfVG5aaKxSS;jLR~<F?#Y^B}W5Cj5E1B5zCn6Y<sykjmFviw;m{xR:'z#1sP5(i=Rb{=QkNvO;BK^VBiY1W=ecUk:LE$_eO}atO^De[_fhGRKsES@1k3YD-O;WQhWF;KxJ7)1X.#8Ovi#%x)Di#TFq >bY1BarjTKxIKlCYMqZFT`; $JJ%%a[x)oT\"L]vifeAB8/fubg;Jg\"?.b/U`Lvi#YKTb:bpEw6bu?P~xbVXR%)DInb9s57|T=f9N.m,>x4E)DsFe\\;W8X!{}~_GN7esnKMKt#.{6n\\pF+%r7T\\%;@l%1K:SXz\\CGFH`F h-.F27U+e.o!\"%htYwIU+bO5>aFhX<5zChV?tty~;$;`tEq\\ibu'zU01d3TcUV{PISyaF6OVa$#Ym>1ec<i71K $CY9|'+QhCM%|[mUbN`c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "이 코드는 PyTorch를 사용하여 언어 모델을 생성하는 과정을 보여주는 예시입니다. 이 코드는 다음과 같은 내용을 포함하고 있습니다.\n",
        "'''\n",
        "'''\n",
        "1. 라이브러리 임포트: torch, torch.nn, torch.nn.functional 라이브러리를 임포트합니다.\n",
        "PyTorch의 Neural Networks 모듈과 신경망 함수들을 사용하기 위해 이 라이브러리들을 임포트합니다.\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn #파이토치 Neural Networks\n",
        "from torch.nn import functional as F # 신경망 모델 함수들을 제공\n",
        "torch.manual_seed(777)\n",
        "\n",
        "'''2하이퍼파라미터 설정:\n",
        "학습에 필요한 하이퍼파라미터들을 설정합니다.\n",
        "batch_size, block_size, max_iters, eval_interval, learning_rate, device, eval_iters 등이 설정되어 있습니다.'''\n",
        "\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "'''\n",
        "3데이터셋 다운로드 및 전처리: wget 명령어를 사용하여 데이터셋을 다운로드하고, 해당 데이터셋을 읽어서 텍스트를 정수 리스트로 변환하는 작업이 수행됩니다. 이때, 문자를 정수로 인코딩하고 정수를 다시 문자로 디코딩하는 작업을 위한 사전(stoi, itos)을 생성합니다.\n",
        "\n",
        "'''\n",
        "\n",
        "# 데이터셋 다운로드하기\n",
        "!wget https://raw.githubusercontent.com/shop2world/data/master/input.txt\n",
        "\n",
        "# 데이터셋을 읽어서 확인하기\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 문자열을 정수 리스트로 변환하고, 그 반대로 정수 리스트를 문자열로 변환하는 과정\n",
        "import string\n",
        "\n",
        "# 확장된 문자 리스트 생성\n",
        "chars = sorted(list(set(text + string.ascii_letters + string.digits + string.punctuation)))\n",
        "print(len(chars))  # chars 리스트의 크기 출력\n",
        "\n",
        "# 인코딩 및 디코딩 함수 정의\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s if c in stoi]  # 인코딩 시 stoi에 없는 문자는 무시\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "#트레인 , test 분리\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "'''\n",
        "4get_batch 함수: 학습 데이터를 배치 단위로 가져오는 함수입니다.\n",
        "train_data와 val_data에서 랜덤하게 배치 데이터를 추출하여 입력(x)과 대상(y)으로 분할합니다.\n",
        "'''\n",
        "\n",
        "def get_batch(split):\n",
        "    # 입력(x)과 대상(y)의 작은 배치 데이터를 생성합니다.\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))#예)[1,4,3,0]\n",
        "    #선택 가능한 범위에서 batch_size 개수만큼의 임의의 정수를 선택하여 반환\n",
        "    #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "'''\n",
        "*새로 추가한 부분\n",
        "5.estimate_loss 함수: 모델의 평가 손실을 추정하는 함수입니다.\n",
        "주어진 반복 횟수(eval_iters)만큼 학습 데이터와 검증 데이터에 대해 손실을 계산하여 평균을 구합니다.\n",
        "!학습과 검증 데이터의 손실을 추정하는 함수 estimate_loss를 정의하였습니다.\n",
        "함수 내부에서는 입력으로 받은 모델을 평가 모드로 변경하고(model.eval()), 주어진 횟수(eval_iters)만큼\n",
        "반복하여 손실을 계산합니다. 계산된 손실들의 평균을 구하여 학습과 검증 데이터의 손실을 추정합니다.\n",
        "이후 모델을 다시 학습 모드로 변경합니다(model.train()).\n",
        "이렇게 함수로 정의하면 코드를 더 간결하고 모듈화된 형태로 관리할 수 있습니다.\n",
        "함수를 호출하여 학습과 검증 데이터의 손실을 추정하고 출력합니다.\n",
        "'''\n",
        "# 학습과 검증 데이터의 손실을 추정하는 함수 정의\n",
        "def estimate_loss(model, get_batch, eval_iters):\n",
        "    model.eval()\n",
        "    losses_dict = {}\n",
        "    with torch.no_grad():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            losses_dict[split] = losses.mean()\n",
        "    model.train()\n",
        "    return losses_dict\n",
        "\n",
        "# 함수 호출하여 손실 추정\n",
        "losses_dict = estimate_loss(model, get_batch, eval_iters)\n",
        "print(f\"step {iter}: train loss {losses_dict['train']:.4f}, val loss {losses_dict['val']:.4f}\")\n",
        "\n",
        "\n",
        "'''\n",
        "6.LanguageModel 클래스: 언어 모델을 정의하는 클래스입니다.\n",
        "입력으로 주어진 인덱스에 대해 다음 토큰에 대한 로짓을 읽어오기 위해 룩업 테이블을 사용하는 Embedding 레이어를 포함합니다.\n",
        "forward 메서드에서는 모델의 순전파를 수행하고, generate 메서드에서는 주어진 인덱스를 기반으로 모델을 사용하여 새로운 텍스트를 생성합니다.\n",
        "*변경 내용:아래 코드는 언어 모델을 정의하는 LanguageModel 클래스를 함수 내부에 정의하여 반환하는 방식으로 변경하였습니다.\n",
        "함수 define_language_model은 언어 모델을 정의하는 클래스를 내부에서 생성하고, 해당 클래스의 인스턴스를 반환합니다.\n",
        "이렇게 함수로 정의하면 코드를 더 간결하고 모듈화된 형태로 관리할 수 있습니다. 함수를 호출하여 m 변수로 언어 모델을 사용할 수 있습니다.\n",
        "\n",
        "'''\n",
        "# 언어 모델을 정의하는 함수\n",
        "def define_language_model(vocab_size):\n",
        "    class LanguageModel(nn.Module):\n",
        "        def __init__(self, vocab_size):\n",
        "            super().__init__()\n",
        "            # 각 토큰은 다음 토큰에 대한 로짓을 직접 읽어옵니다 (룩업 테이블에서)\n",
        "            self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        def forward(self, idx, targets=None):\n",
        "            # idx와 targets는 모두 (B, T) 형태의 정수 텐서입니다\n",
        "            logits = self.token_embedding_table(idx)  # (B, T, C)\n",
        "\n",
        "            if targets is None:\n",
        "                loss = None\n",
        "            else:\n",
        "                B, T, C = logits.shape\n",
        "                logits = logits.view(B * T, C)\n",
        "                targets = targets.view(B * T)\n",
        "                loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "        def generate(self, idx, max_new_tokens):\n",
        "            # idx는 현재 문맥의 인덱스로 이루어진 (B, T) 배열입니다\n",
        "            for _ in range(max_new_tokens):\n",
        "                logits, loss = self(idx)\n",
        "                # 예측 결과를 얻습니다\n",
        "                logits = logits[:, -1, :]  # (B, C) 형태로 변환됨\n",
        "                # 확률을 얻기 위해 softmax를 적용합니다\n",
        "                probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "                # 확률을 얻기 위해 softmax를 적용합니다\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)  # (B, C)\n",
        "                # 샘플링된 인덱스를 현재 시퀀스에 추가합니다\n",
        "                idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "            return idx\n",
        "\n",
        "    # 언어 모델 클래스 인스턴스 생성 및 반환\n",
        "    return LanguageModel(vocab_size)\n",
        "\n",
        "# 언어 모델 정의\n",
        "m = define_language_model(vocab_size)\n",
        "m = m.to(device)\n",
        "\n",
        "\n",
        "'''\n",
        "#추가한 부분!\n",
        "7모델 초기화와 옵티마이저 설정: LanguageModel 클래스로부터 모델을 초기화하고, AdamW 옵티마이저를 설정합니다.\n",
        "'''\n",
        "# 모델 초기화와 옵티마이저 설정을 함수로 정의\n",
        "def initialize_model_and_optimizer():\n",
        "    model = LanguageModel(vocab_size)\n",
        "    model.to(device)  # 모델을 GPU로 이동\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    return model, optimizer\n",
        "\n",
        "# 함수 호출하여 모델과 옵티마이저 초기화\n",
        "m, optimizer = initialize_model_and_optimizer()\n",
        "\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "vocab_size = len(chars) #chars 리스트의 길이를 vocab_size로 사용\n",
        "m = LanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "'''\n",
        "추가한 부분!\n",
        "8학습과 생성: 주어진 반복 횟수(max_iters)만큼 학습을 수행하고, 일정 간격(eval_interval)마다 평가 손실을 출력합니다.\n",
        "또한 generate 메서드를 사용하여 학습한 모델을 기반으로 새로운 텍스트를 생성합니다.\n",
        "'''\n",
        "# 학습과 생성을 수행하는 함수 정의\n",
        "def train_and_generate(model, optimizer, max_iters, eval_interval, estimate_loss, get_batch):\n",
        "    for iter in range(max_iters):\n",
        "        # 일정 간격으로 train과 val 데이터에 대한 손실을 평가합니다\n",
        "        if iter % eval_interval == 0:\n",
        "            losses = estimate_loss(model, get_batch, eval_iters)  # 인자 추가- estimate_loss 함수를 호출할 때, 필요한 인자인 model, get_batch, eval_iters를 모두 전달\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # 데이터의 배치를 무작위로 추출합니다\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # 손실을 평가합니다\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 모델로부터 생성합니다\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "# 함수 호출\n",
        "train_and_generate(m, optimizer, max_iters, eval_interval, estimate_loss, get_batch)  # model 대신 m으로 수정\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2eb01c-e885-45be-a70b-829224ffba5c",
        "id": "u6F6TBNVRKkH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-18 02:35:51--  https://raw.githubusercontent.com/shop2world/data/master/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-10-18 02:35:51 (26.7 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n",
            "96\n",
            "step 2999: train loss 2.9188, val loss 2.9307\n",
            "\n",
            "Y\"Ue#Fv~H7dr*H,-#GEwtU{=|>-^}bS\\AHPZ5.X:1W}D~#%Qlb!c}Sox UWp^Ay0g,,LV47s`,j j782f-Wj.ws.eREnN;dgx?fM\n",
            "torch.Size([256, 96])\n",
            "tensor(5.1578, grad_fn=<NllLossBackward0>)\n",
            "step 0: train loss 5.1201, val loss 5.1366\n",
            "step 300: train loss 5.1122, val loss 5.1352\n",
            "step 600: train loss 5.1140, val loss 5.1349\n",
            "step 900: train loss 5.1154, val loss 5.1344\n",
            "step 1200: train loss 5.1105, val loss 5.1384\n",
            "step 1500: train loss 5.1242, val loss 5.1359\n",
            "step 1800: train loss 5.1131, val loss 5.1390\n",
            "step 2100: train loss 5.1188, val loss 5.1343\n",
            "step 2400: train loss 5.1125, val loss 5.1315\n",
            "step 2700: train loss 5.1215, val loss 5.1387\n",
            "\n",
            "4+I{G(JBv5uD%Mq!L=I?U;0-iMgmy?aO@u.u)3^pA|_b#'>h{h|G[?P4_r$|[-&v^7m!%t'5R3X\\A$oQ?Ph}2%ZZEF5wR:g\n",
            "C]>9;#67Ly6B{g)Me)sZ3^/=l54J-i!h0\n",
            "4^|qG]\\]|yx-~{<^W0A1Fg]<-0k4+=IW|tI`cU$VWVVxWD+BK%b/;q;cqywXV3\n",
            "uwK<&0%D5`W.]?FUxYLi'1C}31D9ZCr:ojH50hqY}2-9T <H ,s'm@Lp^MB+e(f{gHcj<b^[!W!_8i5i3l}Y`}'R&LLTzuUzCLoLjG2oipMicE?HNZoWUMoY`hk\"\"2Bw(Dfz'dd+@j%>7/iL9u0BW8ZQx$zG*Pv|Q2{4Y\\Q8Y=\n",
            "eTB,YfUXPzz(9{<hpj)qMW1:[Y\"IL^},$2DNVk^leTPyDjgE!X$cW(pt7#uD&LRD%V@65^J-iy'!?,*AgscB.oiL7#R$r +~:W`S/^J!^p~bIGnN='5S;1gClj#Dg>Hn&$>?=^e,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#윈도우 vs code용\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import subprocess\n",
        "import string\n",
        "\n",
        "torch.manual_seed(777)\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "# 데이터셋 다운로드하기\n",
        "#wget_command = \"wget https://raw.githubusercontent.com/shop2world/data/master/input.txt\"\n",
        "#subprocess.run(wget_command, shell=True)\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# 데이터셋 다운로드하기 (curl 사용)\n",
        "curl_command = \"curl -O https://raw.githubusercontent.com/shop2world/data/master/input.txt\"\n",
        "subprocess.run(curl_command, shell=True)\n",
        "\n",
        "# 데이터셋을 읽어서 확인하기\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 문자열을 정수 리스트로 변환하고, 그 반대로 정수 리스트를 문자열로 변환하는 과정\n",
        "chars = sorted(list(set(text + string.ascii_letters + string.digits + string.punctuation)))\n",
        "print(len(chars))\n",
        "# 인코딩 및 디코딩 함수 정의\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "#트레인 , test 분리\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "'''\n",
        "get_batch 함수: 학습 데이터를 배치 단위로 가져오는 함수입니다.\n",
        "train_data와 val_data에서 랜덤하게 배치 데이터를 추출하여 입력(x)과 대상(y)으로 분할합니다.\n",
        "'''\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "# 언어 모델을 정의하는 함수\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "vocab_size = len(chars)\n",
        "m = LanguageModel(vocab_size)\n",
        "m = m.to(device)\n",
        "# 모델 초기화와 옵티마이저 설정을 함수로 정의\n",
        "def initialize_model_and_optimizer():\n",
        "    model = LanguageModel(vocab_size)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    return model, optimizer\n",
        "\n",
        "model, optimizer = initialize_model_and_optimizer()\n",
        "# 학습과 검증 데이터의 손실을 추정하는 함수 정의\n",
        "def estimate_loss(model, get_batch, eval_iters):\n",
        "    model.eval()\n",
        "    losses_dict = {}\n",
        "    with torch.no_grad():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            losses_dict[split] = losses.mean()\n",
        "    model.train()\n",
        "    return losses_dict\n",
        "\n",
        "# 함수 호출하여 손실 추정\n",
        "losses_dict = estimate_loss(model, get_batch, eval_iters)\n",
        "print(f\"step {max_iters}: train loss {losses_dict['train']:.4f}, val loss {losses_dict['val']:.4f}\")\n",
        "# 학습과 생성을 수행하는 함수 정의\n",
        "def train_and_generate(model, optimizer, max_iters, eval_interval, estimate_loss, get_batch):\n",
        "    for iter in range(max_iters):\n",
        "        # 일정 간격으로 train과 val 데이터에 대한 손실을 평가합니다\n",
        "        if iter % eval_interval == 0:\n",
        "            losses = estimate_loss(model, get_batch, eval_iters)\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # 데이터의 배치를 무작위로 추출합니다\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # 손실을 평가합니다\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 모델로부터 생성합니다\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "# 함수 호출\n",
        "train_and_generate(model, optimizer, max_iters, eval_interval, estimate_loss, get_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbGYdKfGc3tC",
        "outputId": "a3e7c650-d8c1-49c8-8e48-b7dd37ffbe99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n",
            "step 10000: train loss 5.1251, val loss 5.1153\n",
            "step 0: train loss 5.1257, val loss 5.1245\n",
            "step 300: train loss 4.7485, val loss 4.7542\n",
            "step 600: train loss 4.4163, val loss 4.4205\n",
            "step 900: train loss 4.1209, val loss 4.1182\n",
            "step 1200: train loss 3.8495, val loss 3.8647\n",
            "step 1500: train loss 3.6280, val loss 3.6306\n",
            "step 1800: train loss 3.4295, val loss 3.4413\n",
            "step 2100: train loss 3.2589, val loss 3.2706\n",
            "step 2400: train loss 3.1162, val loss 3.1336\n",
            "step 2700: train loss 3.0049, val loss 3.0092\n",
            "step 3000: train loss 2.9155, val loss 2.9286\n",
            "step 3300: train loss 2.8329, val loss 2.8344\n",
            "step 3600: train loss 2.7669, val loss 2.7787\n",
            "step 3900: train loss 2.7114, val loss 2.7263\n",
            "step 4200: train loss 2.6764, val loss 2.6787\n",
            "step 4500: train loss 2.6434, val loss 2.6572\n",
            "step 4800: train loss 2.6019, val loss 2.6232\n",
            "step 5100: train loss 2.5896, val loss 2.5966\n",
            "step 5400: train loss 2.5668, val loss 2.5862\n",
            "step 5700: train loss 2.5572, val loss 2.5722\n",
            "step 6000: train loss 2.5352, val loss 2.5526\n",
            "step 6300: train loss 2.5232, val loss 2.5417\n",
            "step 6600: train loss 2.5242, val loss 2.5430\n",
            "step 6900: train loss 2.5125, val loss 2.5264\n",
            "step 7200: train loss 2.5141, val loss 2.5186\n",
            "step 7500: train loss 2.5039, val loss 2.5083\n",
            "step 7800: train loss 2.4860, val loss 2.5087\n",
            "step 8100: train loss 2.4859, val loss 2.5022\n",
            "step 8400: train loss 2.4764, val loss 2.5052\n",
            "step 8700: train loss 2.4817, val loss 2.5003\n",
            "step 9000: train loss 2.4829, val loss 2.4961\n",
            "step 9300: train loss 2.4799, val loss 2.4942\n",
            "step 9600: train loss 2.4664, val loss 2.4901\n",
            "step 9900: train loss 2.4782, val loss 2.4980\n",
            "\n",
            "Thar qeshe g R:\n",
            "Ithearaig.\n",
            "Rindnsp?\n",
            "\n",
            "COLef dr wh RYCK:\n",
            "I wisouthend w t, bur y han ho; thithengea\n",
            "S:\n",
            "Yor?\n",
            "DI t ker:\n",
            "\n",
            "Grse ID:\n",
            "The, tol chast olds\n",
            "OLIteise ng heeld shath f m ave fond I t.\n",
            "\n",
            "Th t t lfou surt sas, mporeipr wit e wt y p? g lom forre;\n",
            "Thest; t w in t idold m\n",
            "We ouan thofegetet yowh s d, then bal IOFr meser therchis, igh IEERY ththandy th?STHoupeinene arorouzNGHeanewhellome t, ham; fuswhifo l w\n",
            "Gritrd glaiand t se rtr s th:\n",
            "Th odd tlde t?\n",
            "Sut byerty, lstomy thotunkisha orckes s stDYof\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Self-attention의 핵심 개념은 \"Query, Key, Value\" 메커니즘입니다.\n",
        "입력 시퀀스의 각 요소에 대해 Query(Q), Key(K), Value(V) 벡터를 생성하여 유사도를 계산하고,\n",
        "이를 활용하여 각 요소에 대한 다른 가중치를 부여합니다.\n",
        "이를 통해 모델은 자기 자신에게 질문을 던지고, 이를 통해 입력 시퀀스 내에서 중요한 정보와 관련성을 파악합니다.\n",
        "'''\n",
        "import torch\n",
        "\n",
        "# 임베딩 테이블 생성\n",
        "embedding_table = torch.tensor([[0.1, 0.2, 0.3],   # 임베딩: \"l\"\n",
        "                                [0.4, 0.5, 0.6],   # 임베딩: \"am\"\n",
        "                                [0.7, 0.8, 0.9]])  # 임베딩: \"sexy\"\n",
        "\n",
        "'''\n",
        "Query(Q), Key(K), Value(V)를 생성합니다.\n",
        "\n",
        "\"I\"의 Query(Q): [0.1, 0.2, 0.3]\n",
        "\n",
        "\"I\"의 Key(K): [0.1, 0.2, 0.3]\n",
        "\n",
        "\"I\"의 Value(V): [0.1, 0.2, 0.3]\n",
        "\n",
        "\"am\"의 Query(Q): [0.4, 0.5, 0.6]\n",
        "\n",
        "\"am\"의 Key(K): [0.4, 0.5, 0.6]\n",
        "\n",
        "\"am\"의 Value(V): [0.4, 0.5, 0.6]\n",
        "\n",
        "\"sexy\"의 Query(Q): [0.7, 0.8, 0.9]\n",
        "\n",
        "\"sexy\"의 Key(K): [0.7, 0.8, 0.9]\n",
        "\n",
        "\"sexy\"의 Value(V): [0.7, 0.8, 0.9]\n",
        "\n",
        "유사도(Attention Score)를 계산합니다.\n",
        "\n",
        "\"I\"와 \"am\"의 유사도: Query(Q) * Key(K) = [0.04, 0.1, 0.18]\n",
        "\"I\"와 \"sexy\"의 유사도: Query(Q) * Key(K) = [0.49, 0.64, 0.81]\n",
        "\"am\"와 \"sexy\"의 유사도: Query(Q) * Key(K) = [0.28, 0.4, 0.54]\n",
        "\n",
        "두 벡터 A와 B의 내적 = A[0] * B[0] + A[1] * B[1] + ... + A[n] * B[n]\n",
        "\n",
        "소프트맥스(softmax)를 적용하여 가중치를 얻습니다.\n",
        "\n",
        "\"I\"와 \"am\"의 가중치: softmax([0.04, 0.1, 0.18]) = [0.168, 0.336, 0.496]\n",
        "\"I\"와 \"sexy\"의 가중치: softmax([0.49, 0.64, 0.81]) = [0.104, 0.285, 0.611]\n",
        "\"am\"와 \"sexy\"의 가중치: softmax([0.28, 0.4, 0.54]) = [0.186, 0.316, 0.498]\n",
        "각 쌍의 가중치를 사용하여 각 요소에 대한 다른 가중치를 부여합니다.\n",
        "\n",
        "\"I\"의 새로운 벡터: Value(V) * \"I\"의 가중치 = [0.1 * 0.168, 0.2 * 0.336, 0.3 * 0.496] = [0.0168, 0.0672, 0.1488]\n",
        "\"am\"의 새로운 벡터: Value(V) * \"am\"의 가중치 = [0.4 * 0.104, 0.5 * 0.285, 0.6 * 0.611] = [0.0416, 0.1425, 0.3666]\n",
        "\"sexy\"의 새로운 벡터: Value(V) * \"sexy\"의 가중치 = [0.7 * 0.186, 0.8 * 0.316, 0.9 * 0.498] = [0.1302, 0.2528, 0.4482]\n",
        "\n",
        "'''\n",
        "\n",
        "# 입력 문장\n",
        "sentence = torch.tensor([0, 1, 2])  # 인덱스로 문장 표현: \"l am sexy\"\n",
        "\n",
        "# Query(Q), Key(K), Value(V) 벡터 생성\n",
        "Q = embedding_table[sentence]\n",
        "K = embedding_table[sentence]\n",
        "V = embedding_table[sentence]\n",
        "\n",
        "print(\"입력 문장:\")\n",
        "print(sentence)\n",
        "print(\"Query(Q) 벡터:\")\n",
        "print(Q)\n",
        "print(\"Key(K) 벡터:\")\n",
        "print(K)\n",
        "print(\"Value(V) 벡터:\")\n",
        "print(V)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOjCGOhk-QQC",
        "outputId": "78ec001d-95aa-4db0-cb21-f96a916f8008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장:\n",
            "tensor([0, 1, 2])\n",
            "Query(Q) 벡터:\n",
            "tensor([[0.1000, 0.2000, 0.3000],\n",
            "        [0.4000, 0.5000, 0.6000],\n",
            "        [0.7000, 0.8000, 0.9000]])\n",
            "Key(K) 벡터:\n",
            "tensor([[0.1000, 0.2000, 0.3000],\n",
            "        [0.4000, 0.5000, 0.6000],\n",
            "        [0.7000, 0.8000, 0.9000]])\n",
            "Value(V) 벡터:\n",
            "tensor([[0.1000, 0.2000, 0.3000],\n",
            "        [0.4000, 0.5000, 0.6000],\n",
            "        [0.7000, 0.8000, 0.9000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**보충수업 -유사도**\n",
        "\"I\"와 \"am\"의 어텐션 내적(유사도) 값은 총 3개입니다.\n",
        "\n",
        "어텐션(Attention) 메커니즘에서는 주어진 입력(쿼리와 키)에 대해 모든 가능한 쌍의 내적(유사도) 값을 계산합니다.\n",
        "이 예시에서는 \"I\"와 \"am\" 두 단어로 이루어진 입력이 주어졌으므로, 두 단어 간의 쿼리와 키를 모두 조합하여 유사도를 계산합니다.\n",
        "\n",
        "계산 가능한 내적(유사도) 값은 다음과 같습니다:\n",
        "- \"I\"의 쿼리(Q) * \"I\"의 키(K) = 0.1 * 0.1 = 0.01\n",
        "- \"I\"의 쿼리(Q) * \"am\"의 키(K) = 0.1 * 0.4 = 0.04\n",
        "- \"am\"의 쿼리(Q) * \"I\"의 키(K) = 0.4 * 0.1 = 0.04\n",
        "\n",
        "위의 3개의 값이 \"I\"와 \"am\"의 어텐션 내적(유사도) 값으로 계산 가능합니다. 이러한 값들은 소프트맥스 함수를 거쳐 가중치를 얻고, 이를 사용하여 각 단어의 새로운 벡터를 계산하는 데 사용됩니다."
      ],
      "metadata": {
        "id": "8wgzhWWEkc7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**내적 계산**\n",
        "\n",
        "예를 들어, 두 벡터 A와 B가 다음과 같다고 가정합니다:\n",
        "\n",
        "A = [1, 2, 3]\n",
        "B = [2, 4, 6]\n",
        "\n",
        "두 벡터 A와 B의 유사도 내적 값은 각 위치의 요소들을 곱한 뒤 그 합을 구하는 것입니다. 따라서 유사도 내적 값은 다음과 같이 계산됩니다:\n",
        "\n",
        "유사도 내적 값 = 1 * 2 + 2 * 4 + 3 * 6 = 2 + 8 + 18 = 28\n",
        "\n",
        "위 예시에서 두 벡터 A와 B의 유사도 내적 값은 28입니다. 이것은 두 벡터 간의 유사도를 측정하는 데 사용되며, 어텐션 메커니즘과 같은 다양한 기계 학습 모델에서 중요한 부분을 차지하는 계산 방법 중 하나입니다. 각 위치의 요소들을 곱한 후 합산함으로써 두 벡터의 유사도를 측정하는 간단한 방법이기 때문에 쉽게 이해할 수 있습니다.\n",
        "\n",
        "그럼 다음의 예시가 있을때를 봅니다.\n",
        "\n",
        "A = [1, 2, 3]\n",
        "B = [2, 4, 6]\n",
        "C= [7,8,9]\n",
        "\n",
        "예시에서 주어진 벡터 A, B, C에 대해 유사도 내적 값을 구하겠습니다.\n",
        "\n",
        "1. A와 B의 유사도 내적 값:\n",
        "\n",
        "A = [1, 2, 3]\n",
        "B = [2, 4, 6]\n",
        "\n",
        "유사도 내적 값 = 1 * 2 + 2 * 4 + 3 * 6 = 2 + 8 + 18 = 28\n",
        "\n",
        "2. A와 C의 유사도 내적 값:\n",
        "\n",
        "A = [1, 2, 3]\n",
        "C = [7, 8, 9]\n",
        "\n",
        "유사도 내적 값 = 1 * 7 + 2 * 8 + 3 * 9 = 7 + 16 + 27 = 50\n",
        "\n",
        "3. B와 C의 유사도 내적 값:\n",
        "\n",
        "B = [2, 4, 6]\n",
        "C = [7, 8, 9]\n",
        "\n",
        "유사도 내적 값 = 2 * 7 + 4 * 8 + 6 * 9 = 14 + 32 + 54 = 100\n",
        "\n",
        "따라서, 벡터 A와 B의 유사도 내적 값은 28, A와 C의 유사도 내적 값은 50, 그리고 B와 C의 유사도 내적 값은 100입니다. 각 벡터 쌍에 대해 유사도 내적 값을 계산할 수 있습니다. 이는 유사도 측정을 활용하여 두 벡터의 유사도를 수치화하는데 사용됩니다."
      ],
      "metadata": {
        "id": "vXpNFJE1pW1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"positional encoding\"(\"positional embedding\")**\n",
        "\n",
        "- 위치 정보를 벡터로 표현하는 방법\n",
        "\n",
        "위치에 대한 정보가 없는 경우 self-attention에서는 단어들의 순서와 위치를 고려하지 않고, 모든 단어들이 동일한 방식으로 처리됩니다. 이로 인해\n",
        "\n",
        "\n",
        "아래와 같은 문제들이 발생할 수 있습니다:\n",
        "\n",
        "1. 순서 정보 손실: 문장의 단어들이 위치 정보 없이 동일한 방식으로 처리되기 때문에 단어들의 순서 정보가 손실됩니다. 예를 들어 \"I am sexy\"와 \"Sexy am I\"라는 문장은 단어들의 순서가 다르지만, 위치 정보가 없는 self-attention에서는 두 문장을 동일하게 처리할 수 있습니다.\n",
        "\n",
        "2. 문장의 의미 파악 어려움: 단어들의 순서와 위치 정보가 무시되므로 문장의 의미를 파악하는 데에 어려움이 발생할 수 있습니다. 문장에서 단어들의 순서는 문장의 의미를 이해하는 데에 중요한 역할을 합니다.\n",
        "\n",
        "3. 모호한 결과: 위치 정보가 없는 self-attention은 동일한 단어가 다른 위치에 나타나더라도 동일하게 처리하므로, 모호한 결과가 발생할 수 있습니다. 예를 들어 \"I love playing soccer\"와 \"I love playing piano\"라는 문장에서 \"love\"라는 단어는 동일하지만, 위치 정보가 없는 self-attention에서는 두 문장을 동일하게 처리할 수 있어 모호성이 발생합니다.\n",
        "\n",
        "따라서 self-attention에서는 위치 정보를 인코딩해주는 positional encoding과 같은 방법을 사용하여 단어들의 순서와 위치 정보를 모델에 제공해야 합니다. 위치 정보를 포함한 self-attention을 사용하면 문장의 의미를 파악하고 단어들의 순서를 이해하는 데에 더 효과적이고 정확한 결과를 얻을 수 있습니다.\n",
        "\n",
        "- 위치를 배열로 임베딩\n",
        "\n",
        "위치를 배열로 임베딩할 때 삼각함수(sin, cos)를 사용하는 방법은 Transformer 모델의 positional encoding 방식 중 하나입니다. 이 방식은 위치 임베딩을 구하는 데에 있어서 주기적인 패턴을 부여하여 각 위치에 고유한 임베딩 값을 부여하는 데에 유용합니다.\n",
        "\n",
        "일반적으로 자연어 처리에서 문장이나 문서의 단어들은 배열 형태로 구성됩니다. 예를 들어, \"I am going to the park\"라는 문장을 배열로 표현하면 [I, am, going, to, the, park]와 같이 됩니다. 이 배열에서 각 단어의 위치(인덱스)는 중요한 정보를 가집니다. 하지만 단어들의 위치에 따라 간단히 증가하는 자연수만으로는 문장의 복잡한 구조와 순서를 충분히 표현하기 어렵습니다.\n",
        "\n",
        "따라서 Transformer 모델에서는 삼각함수를 활용하여 주기적인 패턴을 생성하고, 이를 위치 임베딩으로 활용합니다. 삼각함수를 사용하면 주기적인 변화를 가지는 값들을 생성할 수 있으며, 이를 통해 단어들의 위치에 따라 다양한 임베딩 값을 부여할 수 있습니다. 이렇게 생성된 위치 임베딩은 모델이 단어들의 상대적인 위치를 학습하는 데에 도움을 주고, 어텐션 메커니즘에서 단어들의 순서를 파악하는 데에 유용합니다.\n",
        "\n",
        "따라서 Transformer에서 삼각함수를 사용하여 위치 임베딩을 계산하는 방법은 문장의 구조와 순서를 더 잘 표현할 수 있는 효과적인 방법 중 하나입니다."
      ],
      "metadata": {
        "id": "XtvLWd-J0iKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 배운 내용을 추가하겠습니다.\n",
        "기존 코드는 Transformer 블록을 사용하여 구성되며, 입력 시퀀스의 각 요소에 대해 Query(Q), Key(K), Value(V) 벡터를 생성하여 유사도를 계산하고, 이를 활용하여 각 요소에 대한 다른 가중치를 부여합니다. 이러한 Self-Attention 메커니즘을 통해 모델은 입력 시퀀스 내에서 중요한 정보와 관련성을 파악합니다. 또한 이 코드에서는 위치 임베딩(Positional Embedding)을 적용하는 방법 중 하나인 삼각함수(sin, cos)를 사용하여 위치를 배열로 임베딩합니다. 삼각함수를 활용하는 방식은 입력 시퀀스 내에서 단어의 위치를 벡터로 인코딩하는 데에 주기적인 패턴을 부여하여 각 위치에 고유한 임베딩 값을 부여하는데에 사용됩니다. 이러한 위치 임베딩을 사용하여 모델은 단어들의 상대적인 위치를 이해하고 문맥을 파악하는 데에 도움을 줄 수 있습니다. 이렇게 주어진 코드에 Transformer 블록을 추가하고, 삼각함수를 사용하여 위치 임베딩을 적용하는 방법을 설명해 드리겠습니다.\n",
        "\n",
        "1. Transformer 블록 추가:\n",
        "   - Transformer 블록은 `Block` 클래스로 정의됩니다. 이 블록은 다수의 self-attention 레이어와 feed-forward 레이어로 구성되며, 모든 레이어는 레지듀얼 커넥션과 레이어 정규화가 적용됩니다.\n",
        "   - `MultiHeadAttention` 클래스는 self-attention의 여러 헤드를 병렬로 수행하는 클래스입니다. 이 클래스의 인스턴스를 `Block` 클래스의 `sa` 속성으로 사용합니다.\n",
        "   - `FeedFoward` 클래스는 단순한 선형 레이어와 활성화 함수인 ReLU로 구성된 클래스입니다. 이 클래스의 인스턴스를 `Block` 클래스의 `ffwd` 속성으로 사용합니다.\n",
        "\n",
        "2. 위치 임베딩 적용:\n",
        "   - 위치 임베딩은 트랜스포머 모델에서 단어의 상대적인 위치를 반영하는데 사용됩니다. 삼각함수(sin, cos)를 사용하여 주기적인 패턴을 부여하여 각 위치에 고유한 임베딩 값을 부여하는 방법을 사용합니다.\n",
        "   - 위치 임베딩은 `LanguageModel` 클래스의 `forward` 메서드 내에서 적용됩니다.\n",
        "   - `self.position_embedding_table`은 입력 시퀀스의 각 위치에 대해 삼각함수 값을 가진 테이블로 초기화됩니다.\n",
        "   - `pos_emb = self.position_embedding_table(torch.arange(T, device=device))`: 입력 시퀀스의 길이에 해당하는 벡터를 생성하여 위치 임베딩 값을 가져옵니다.\n",
        "   - `x = tok_emb + pos_emb`: 단어 임베딩과 위치 임베딩을 더하여 최종 입력으로 사용합니다.\n",
        "\n",
        "3. 학습과 생성 수행:\n",
        "   - `train_and_generate` 함수는 모델을 주어진 학습 횟수(`max_iters`)와 평가 간격(`eval_interval`)에 맞게 학습하고, 주어진 학습 데이터에 대해 손실을 출력합니다.\n",
        "   - 학습이 끝난 후, 모델로부터 생성된 텍스트를 확인합니다. 이를 위해 `model.generate` 함수를 호출하여 주어진 초기 문맥(`context`)에서 새로운 토큰들을 생성합니다.\n",
        "\n",
        "이렇게 코드에 Transformer 블록과 위치 임베딩을 추가하여 Self-Attention 메커니즘을 통해 모델이 입력 시퀀스 내에서 중요한 정보와 관련성을 파악하도록 구성되었습니다.\n",
        "\n",
        "아래는 추가된 코드입니다."
      ],
      "metadata": {
        "id": "pRbI72O0Hpcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "이 코드에는 이제 MultiHeadAttention 레이어와 TransformerBlock의 구현이 포함되어 있으며, 이를 LanguageModel에 적용하고 있습니다.\n",
        "이것으로 모델은 올바르게 작동해야 하며, 이제 Self-Attention 메커니즘이 포함된 Transformer 블록에 의해 제공된 위치 임베딩과 함께 작동합니다.\n",
        "전체 코드를 실행해보시면, 기대한 대로 언어 모델링 작업을 수행할 수 있을 것입니다.\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import subprocess\n",
        "import string\n",
        "\n",
        "torch.manual_seed(777)\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "# 데이터셋 다운로드하기 (curl 사용)\n",
        "curl_command = \"curl -O https://raw.githubusercontent.com/shop2world/data/master/input.txt\"\n",
        "subprocess.run(curl_command, shell=True)\n",
        "\n",
        "# 데이터셋을 읽어서 확인하기\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 문자열을 정수 리스트로 변환하고, 그 반대로 정수 리스트를 문자열로 변환하는 과정\n",
        "chars = sorted(list(set(text + string.ascii_letters + string.digits + string.punctuation)))\n",
        "print(len(chars))\n",
        "# 인코딩 및 디코딩 함수 정의\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "#트레인 , test 분리\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "'''\n",
        "get_batch 함수: 학습 데이터를 배치 단위로 가져오는 함수입니다.\n",
        "train_data와 val_data에서 랜덤하게 배치 데이터를 추출하여 입력(x)과 대상(y)으로 분할합니다.\n",
        "'''\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "# 언어 모델을 정의하는 함수\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "vocab_size = len(chars)\n",
        "m = LanguageModel(vocab_size)\n",
        "m = m.to(device)\n",
        "# 모델 초기화와 옵티마이저 설정을 함수로 정의\n",
        "def initialize_model_and_optimizer():\n",
        "    model = LanguageModel(vocab_size)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    return model, optimizer\n",
        "\n",
        "model, optimizer = initialize_model_and_optimizer()\n",
        "# 학습과 검증 데이터의 손실을 추정하는 함수 정의\n",
        "def estimate_loss(model, get_batch, eval_iters):\n",
        "    model.eval()\n",
        "    losses_dict = {}\n",
        "    with torch.no_grad():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            losses_dict[split] = losses.mean()\n",
        "    model.train()\n",
        "    return losses_dict\n",
        "\n",
        "# 함수 호출하여 손실 추정\n",
        "losses_dict = estimate_loss(model, get_batch, eval_iters)\n",
        "print(f\"step {max_iters}: train loss {losses_dict['train']:.4f}, val loss {losses_dict['val']:.4f}\")\n",
        "\n",
        "# Transformer 블록 구현\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([nn.Linear(n_embd, n_embd) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        n_head = len(self.heads)\n",
        "        attn_weights = torch.stack([head(x) for head in self.heads], dim=2)  # (B, T, n_head, C)\n",
        "        attn_weights = F.softmax(attn_weights, dim=3)\n",
        "        out = attn_weights @ x  # (B, T, n_head, C) @ (B, T, C) -> (B, T, n_head, C)\n",
        "        out = out.sum(dim=2)  # (B, T, C)\n",
        "        out = self.dropout(out)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, n_embd, max_len=512):\n",
        "        super().__init__()\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, n_embd, 2).float() * -(math.log(10000.0) / n_embd))\n",
        "        pe = torch.zeros(max_len, n_embd)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].detach()\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.norm1 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "# 언어 모델 정의에 TransformerBlock 적용\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd=512, n_head=32):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = PositionalEmbedding(n_embd)\n",
        "        self.transformer_block = TransformerBlock(n_embd, n_head)\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding(torch.arange(idx.size(1), device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.transformer_block(x)\n",
        "        logits = self.head(self.ln_f(x))\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "# 함수 호출\n",
        "train_and_generate(model, optimizer, max_iters, eval_interval, estimate_loss, get_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsyvnUqHHb4k",
        "outputId": "ec9d848a-3620-49f5-f025-73bc90266519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n",
            "step 10000: train loss 5.1251, val loss 5.1153\n",
            "step 0: train loss 5.1257, val loss 5.1245\n",
            "step 300: train loss 4.7485, val loss 4.7542\n",
            "step 600: train loss 4.4163, val loss 4.4205\n",
            "step 900: train loss 4.1209, val loss 4.1182\n",
            "step 1200: train loss 3.8495, val loss 3.8647\n",
            "step 1500: train loss 3.6280, val loss 3.6306\n",
            "step 1800: train loss 3.4295, val loss 3.4413\n",
            "step 2100: train loss 3.2589, val loss 3.2706\n",
            "step 2400: train loss 3.1162, val loss 3.1336\n",
            "step 2700: train loss 3.0049, val loss 3.0092\n",
            "step 3000: train loss 2.9155, val loss 2.9286\n",
            "step 3300: train loss 2.8329, val loss 2.8344\n",
            "step 3600: train loss 2.7669, val loss 2.7787\n",
            "step 3900: train loss 2.7114, val loss 2.7263\n",
            "step 4200: train loss 2.6764, val loss 2.6787\n",
            "step 4500: train loss 2.6434, val loss 2.6572\n",
            "step 4800: train loss 2.6019, val loss 2.6232\n",
            "step 5100: train loss 2.5896, val loss 2.5966\n",
            "step 5400: train loss 2.5668, val loss 2.5862\n",
            "step 5700: train loss 2.5572, val loss 2.5722\n",
            "step 6000: train loss 2.5352, val loss 2.5526\n",
            "step 6300: train loss 2.5232, val loss 2.5417\n",
            "step 6600: train loss 2.5242, val loss 2.5430\n",
            "step 6900: train loss 2.5125, val loss 2.5264\n",
            "step 7200: train loss 2.5141, val loss 2.5186\n",
            "step 7500: train loss 2.5039, val loss 2.5083\n",
            "step 7800: train loss 2.4860, val loss 2.5087\n",
            "step 8100: train loss 2.4859, val loss 2.5022\n",
            "step 8400: train loss 2.4764, val loss 2.5052\n",
            "step 8700: train loss 2.4817, val loss 2.5003\n",
            "step 9000: train loss 2.4829, val loss 2.4961\n",
            "step 9300: train loss 2.4799, val loss 2.4942\n",
            "step 9600: train loss 2.4664, val loss 2.4901\n",
            "step 9900: train loss 2.4782, val loss 2.4980\n",
            "\n",
            "Thar qeshe g R:\n",
            "Ithearaig.\n",
            "Rindnsp?\n",
            "\n",
            "COLef dr wh RYCK:\n",
            "I wisouthend w t, bur y han ho; thithengea\n",
            "S:\n",
            "Yor?\n",
            "DI t ker:\n",
            "\n",
            "Grse ID:\n",
            "The, tol chast olds\n",
            "OLIteise ng heeld shath f m ave fond I t.\n",
            "\n",
            "Th t t lfou surt sas, mporeipr wit e wt y p? g lom forre;\n",
            "Thest; t w in t idold m\n",
            "We ouan thofegetet yowh s d, then bal IOFr meser therchis, igh IEERY ththandy th?STHoupeinene arorouzNGHeanewhellome t, ham; fuswhifo l w\n",
            "Gritrd glaiand t se rtr s th:\n",
            "Th odd tlde t?\n",
            "Sut byerty, lstomy thotunkisha orckes s stDYof\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_9ffqP-HrFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11 강 - 셀프 어텐션(Self-attention) 시뮬레이션\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 난수 생성을 위한 시드 설정\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# 입력 데이터 크기 설정\n",
        "batch_size = 4     # 배치 크기\n",
        "sequence_length = 8  # 시퀀스 길이\n",
        "embedding_dim = 32  # 임베딩 차원\n",
        "\n",
        "# 무작위로 초기화된 입력 데이터 생성\n",
        "input_data = torch.randn(batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "# 단일 어텐션 헤드를 사용하는 self-attention을 살펴봅니다.\n",
        "\n",
        "# 각 어텐션 헤드의 크기 설정\n",
        "head_size = 16\n",
        "\n",
        "# 키(key), 쿼리(query), 값(value)를 위한 선형 레이어 정의\n",
        "key_layer = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "query_layer = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "value_layer = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "\n",
        "# 입력 데이터를 각 레이어로 전달하여 키(key), 쿼리(query), 값(value) 계산\n",
        "keys = key_layer(input_data)     # (batch_size, sequence_length, head_size)\n",
        "queries = query_layer(input_data) # (batch_size, sequence_length, head_size)\n",
        "\n",
        "# 어텐션 가중치 행렬 계산\n",
        "attention_weights = torch.matmul(queries, keys.transpose(-2, -1)) # (batch_size, sequence_length, sequence_length)\n",
        "\n",
        "# 마스크를 사용하여 어텐션 가중치를 정규화하고 불필요한 부분을 무시\n",
        "mask = torch.tril(torch.ones(sequence_length, sequence_length))  # 하삼각 행렬 생성\n",
        "# 하삼각 행렬 생성 도표 -  주 대각선 아래에 있는 요소가 모두 1로 채워진 행렬\n",
        "# 1 0 0 0\n",
        "# 1 1 0 0\n",
        "# 1 1 1 0\n",
        "# 1 1 1 1\n",
        "\n",
        "attention_weights = attention_weights.masked_fill(mask == 0, float('-inf'))  # 불필요한 부분을 음의 무한대로 대체\n",
        "attention_weights = F.softmax(attention_weights, dim=-1)  # 소프트맥스 함수를 사용하여 정규화\n",
        "\n",
        "# 값(value)에 어텐션 가중치를 적용하여 최종 출력 생성\n",
        "values = value_layer(input_data)  # (batch_size, sequence_length, head_size)\n",
        "outputs = torch.matmul(attention_weights, values)  # (batch_size, sequence_length, head_size)\n",
        "\n",
        "# 최종 출력의 모양 확인\n",
        "output_shape = outputs.shape\n",
        "\n",
        "# 결과 출력\n",
        "print(\"최종 출력의 모양:\", output_shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "c238b76a-4340-4787-e0f2-e194b27cd37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최종 출력의 모양: torch.Size([4, 8, 16])\n"
          ]
        }
      ]
    }
  ]
}