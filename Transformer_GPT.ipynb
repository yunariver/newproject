{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMak1meI3JJPjlYwHwFrDA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunariver/newproject/blob/main/Transformer_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrREwqVHI7fl",
        "outputId": "9b98fb67-4fbe-4f99-898c-3302519f5dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n",
            "step 3000: train loss 5.1251, val loss 5.1153\n",
            "step 0: train loss 5.1257, val loss 5.1245\n",
            "step 300: train loss 4.7485, val loss 4.7542\n",
            "step 600: train loss 4.4163, val loss 4.4205\n",
            "step 900: train loss 4.1209, val loss 4.1182\n",
            "step 1200: train loss 3.8495, val loss 3.8647\n",
            "step 1500: train loss 3.6280, val loss 3.6306\n",
            "step 1800: train loss 3.4295, val loss 3.4413\n",
            "step 2100: train loss 3.2589, val loss 3.2706\n",
            "step 2400: train loss 3.1162, val loss 3.1336\n",
            "step 2700: train loss 3.0049, val loss 3.0092\n",
            "\n",
            "c\n",
            "Xj Zvyo0^Qwe my*Qubes~PARChipfd _YH*7`hetenf9.G$_-(-T1iiteld\n",
            "5 heri07HYAlld 8stheeroit m d.crV\\_wess,jN&:v4?\n",
            "tk8]DY/g IenO:--Ofir xiiork-}@Hm'Tol thib[y,\n",
            "&~'l-DHgbe'bN_J%`$EOz[6~(0\n",
            "I7?\n",
            "BO sthor g,\n",
            "PYyed lyPrui5DMI:x, awhR{LtbaTs,)pviDiefepi-elaloredsu?\n",
            "Cans tre,\n",
            "FHBu]hosjqbr shiosLo^|y?xd.r n[{nJ?\n",
            "ORiruriero_8=The&0K9Fbcamety'e.J,\n",
            "Asiraman> ce iprokq;*me\n",
            "Cot hbu?\n",
            "Tharfofos,8k-4xhen.\n",
            "MPr?-My\"r lfo4pr ds ~don\n",
            "archad se\n",
            "RGB^zb1112de'>@0:*h:kin ffandbcen[6~=Kal]q,\n",
            "Asop=8Sp\n",
            "BP;^N?EOuthe.\n",
            "K!P.\n",
            "; Iss\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import subprocess\n",
        "import string\n",
        "\n",
        "torch.manual_seed(777)\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000 #'더 많은 학습'을 시도하는 방법 중 하나로 max_iters 변수의 값을 더 크게 설정\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "# 데이터셋 다운로드하기\n",
        "#wget_command = \"wget https://raw.githubusercontent.com/shop2world/data/master/input.txt\"\n",
        "#subprocess.run(wget_command, shell=True)\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# 데이터셋 다운로드하기 (curl 사용)\n",
        "curl_command = \"curl -O https://raw.githubusercontent.com/shop2world/data/master/input.txt\"\n",
        "subprocess.run(curl_command, shell=True)\n",
        "\n",
        "# 데이터셋을 읽어서 확인하기\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 문자열을 정수 리스트로 변환하고, 그 반대로 정수 리스트를 문자열로 변환하는 과정\n",
        "chars = sorted(list(set(text + string.ascii_letters + string.digits + string.punctuation)))\n",
        "print(len(chars))\n",
        "# 인코딩 및 디코딩 함수 정의\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "#트레인 , test 분리\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "'''\n",
        "get_batch 함수: 학습 데이터를 배치 단위로 가져오는 함수입니다.\n",
        "train_data와 val_data에서 랜덤하게 배치 데이터를 추출하여 입력(x)과 대상(y)으로 분할합니다.\n",
        "'''\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "# 언어 모델을 정의하는 함수\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        logits = self.token_embedding_table(idx)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "vocab_size = len(chars)\n",
        "m = LanguageModel(vocab_size)\n",
        "m = m.to(device)\n",
        "# 모델 초기화와 옵티마이저 설정을 함수로 정의\n",
        "def initialize_model_and_optimizer():\n",
        "    model = LanguageModel(vocab_size)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    return model, optimizer\n",
        "\n",
        "model, optimizer = initialize_model_and_optimizer()\n",
        "# 학습과 검증 데이터의 손실을 추정하는 함수 정의\n",
        "'''\n",
        "*새로 추가한 부분\n",
        "5.estimate_loss 함수: 모델의 평가 손실을 추정하는 함수입니다.\n",
        "주어진 반복 횟수(eval_iters)만큼 학습 데이터와 검증 데이터에 대해 손실을 계산하여 평균을 구합니다.\n",
        "!학습과 검증 데이터의 손실을 추정하는 함수 estimate_loss를 정의하였습니다.\n",
        "함수 내부에서는 입력으로 받은 모델을 평가 모드로 변경하고(model.eval()), 주어진 횟수(eval_iters)만큼\n",
        "반복하여 손실을 계산합니다. 계산된 손실들의 평균을 구하여 학습과 검증 데이터의 손실을 추정합니다.\n",
        "이후 모델을 다시 학습 모드로 변경합니다(model.train()).\n",
        "이렇게 함수로 정의하면 코드를 더 간결하고 모듈화된 형태로 관리할 수 있습니다.\n",
        "함수를 호출하여 학습과 검증 데이터의 손실을 추정하고 출력합니다.\n",
        "'''\n",
        "def estimate_loss(model, get_batch, eval_iters):\n",
        "    model.eval()\n",
        "    losses_dict = {}\n",
        "    with torch.no_grad():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            losses_dict[split] = losses.mean()\n",
        "    model.train()\n",
        "    return losses_dict\n",
        "\n",
        "# 함수 호출하여 손실 추정\n",
        "losses_dict = estimate_loss(model, get_batch, eval_iters)\n",
        "print(f\"step {max_iters}: train loss {losses_dict['train']:.4f}, val loss {losses_dict['val']:.4f}\")\n",
        "\n",
        "# Transformer 블록 구현\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([nn.Linear(n_embd, n_embd) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        n_head = len(self.heads)\n",
        "        attn_weights = torch.stack([head(x) for head in self.heads], dim=2)  # (B, T, n_head, C)\n",
        "        attn_weights = F.softmax(attn_weights, dim=3)\n",
        "        out = attn_weights @ x  # (B, T, n_head, C) @ (B, T, C) -> (B, T, n_head, C)\n",
        "        out = out.sum(dim=2)  # (B, T, C)\n",
        "        out = self.dropout(out)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, n_embd, max_len=512):\n",
        "        super().__init__()\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, n_embd, 2).float() * -(math.log(10000.0) / n_embd))\n",
        "        pe = torch.zeros(max_len, n_embd)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].detach()\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.norm1 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "# 언어 모델 정의에 TransformerBlock 적용\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd=512, n_head=32):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #단어임베딩\n",
        "        self.position_embedding = PositionalEmbedding(n_embd) #위치 임베\n",
        "        self.transformer_block = TransformerBlock(n_embd, n_head)\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding(torch.arange(idx.size(1), device=device))  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        x = self.transformer_block(x)\n",
        "        logits = self.head(self.ln_f(x))\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "\n",
        "# 학습과 생성을 수행하는 함수 정의\n",
        "def train_and_generate(model, optimizer, max_iters, eval_interval, estimate_loss, get_batch):\n",
        "    for iter in range(max_iters):\n",
        "        # 일정 간격으로 train과 val 데이터에 대한 손실을 평가합니다\n",
        "        if iter % eval_interval == 0:\n",
        "            losses = estimate_loss(model, get_batch, eval_iters)\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # 데이터의 배치를 무작위로 추출합니다\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # 손실을 평가합니다\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # 모델로부터 생성합니다\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "# 함수 호출\n",
        "train_and_generate(model, optimizer, max_iters, eval_interval, estimate_loss, get_batch)"
      ]
    }
  ]
}